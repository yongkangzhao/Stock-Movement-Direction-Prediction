{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is still a very early version, and still working on making a framework that will get basic things going. Will enhance and polish all areas afterwards.\n",
    "# todo:\n",
    "#   add more features\n",
    "#   add more data sources, options, vix index, \n",
    "#   implement bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from marketstackAPI import Marketstack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cufflinks as cf\n",
    "import ta\n",
    "import holidays\n",
    "import matplotlib as plt\n",
    "import plotly.graph_objects as go\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# jupyter notebook settings and chart size configs\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "plt.rcParams['figure.figsize'] = [12, 5]\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize and set parameters\n",
    "MS = Marketstack() # requires API key from Marketstack with basic plan to get 10 years worth of data\n",
    "cf.set_config_file(theme='henanigans',sharing='public',offline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data_preprocessing(raw_data):\n",
    "    \"\"\"\n",
    "    Clean raw_data by removing extra columns, renaming columns, order by date in descending order, reset index number.\n",
    "    this data format will be used as the standard format for all other feature engineering related function calls.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_data : pandas dataframe that contains ['date','adj_high','adj_low','adj_close','adj_open','adj_volume'] columns, ordered by date in ascending order.\n",
    "    \n",
    "    Return:\n",
    "    ----------\n",
    "    standard_data: pandas dataframe that contains ['date','high','low','close','open','volume'] columns, ordered by date in descending order.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = raw_data[:]\n",
    "    data = data[['date','adj_high','adj_low','adj_close','adj_open','adj_volume']]\n",
    "    data.columns = ['date','high','low','close','open','volume']\n",
    "    data = data[::-1]\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ta_indicators(standard_data, prefix = ''):\n",
    "    \"\"\"\n",
    "    Compute technical indicators for every period, each row within standard_data is a period.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    standard_data : pandas dataframe that contains ['date','high','low','close','open','volume'] columns, ordered by date in descending order.\n",
    "    prefix: string that will be concatennated to before all technical indicators names\n",
    "    \n",
    "    Return:\n",
    "    ----------\n",
    "    data: pandas dataframe that contains computed technical indicators with corrsponding name\n",
    "    \n",
    "    \"\"\"\n",
    "    data = standard_data[:]\n",
    "    df = pd.DataFrame()\n",
    "    df.insert(0, prefix+'_stochrsi_14' if prefix else 'stochrsi_14', ta.momentum.stochrsi(close = data.close)/100) # range 0 to 100 rescaled to 0 to 1\n",
    "    df.insert(0, prefix+'_mfi_14' if prefix else 'mfi_14', ta.volume.money_flow_index(high = data.high, low = data.low, close = data.close, volume= data.volume)/100) # range 0 to 100 rescaled to 0 to 1\n",
    "    df.insert(0, prefix+'_adx_14' if prefix else 'adx_14', ta.trend.adx(high = data.high, low = data.low, close = data.close)/100) # range 0 to 100 rescaled to 0 to 1\n",
    "    df.insert(0, prefix+'_adx_neg_14' if prefix else 'adx_neg_14', ta.trend.adx_neg(high = data.high, low = data.low, close = data.close)/100) # range 0 to 100 rescaled to 0 to 1\n",
    "    df.insert(0, prefix+'_adx_pos_14' if prefix else 'adx_pos_14', ta.trend.adx_pos(high = data.high, low = data.low, close = data.close)/100) # range 0 to 100 rescaled to 0 to 1\n",
    "    df.insert(0, prefix+'_aroon_up_25' if prefix else 'aroon_up_25', ta.trend.aroon_up(close = data.close)/100) # range 0 to 100 rescaled to 0 to 1\n",
    "    df.insert(0, prefix+'_aroon_down_25' if prefix else 'aroon_down_25', ta.trend.aroon_down(close = data.close)/100) # range 0 to 100 rescaled to 0 to 1\n",
    "    df.insert(0, prefix+'_aroon_25' if prefix else 'aroon_25', (ta.trend.aroon_up(close = data.close) - ta.trend.aroon_down(close = data.close))/100) # range 0 to 100 rescaled to 0 to 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_changes(standard_data, prefix = ''):\n",
    "    \"\"\"\n",
    "    Compute basic % changes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    standard_data : pandas dataframe that contains ['date','high','low','close','open','volume'] columns, ordered by date in descending order.\n",
    "    prefix: string that will be concatennated to before all technical indicators names\n",
    "    \n",
    "    Return:\n",
    "    ----------\n",
    "    data: pandas dataframe that contains computed % changes indicators with corrsponding name\n",
    "    \n",
    "    \"\"\"\n",
    "    data = standard_data[:]\n",
    "    df = pd.DataFrame()\n",
    "    #add volume % change from yesterday to today\n",
    "    df.insert(0,'volume_change',data.volume/data.volume.shift(1)-1)\n",
    "    #add price % change from yesterday to today\n",
    "    df.insert(0,'price_change',data.close/data.close.shift(1)-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_variable(standard_data):\n",
    "    \"\"\"\n",
    "    Compute target variable.\n",
    "    the target variable indicates three classes.\n",
    "    2 : next day is going up significantly\n",
    "    0: next day is going down significantly\n",
    "    1 : no significant movement for the next day.\n",
    "    \n",
    "    How significant change is defined using more than 1% change at the moment. could be changing to something else.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    standard_data : pandas dataframe that contains ['date','high','low','close','open','volume'] columns, ordered by date in descending order.\n",
    "    \n",
    "    Return:\n",
    "    ----------\n",
    "    data: pandas dataframe that contains target variable\n",
    "    \n",
    "    \"\"\"\n",
    "    data = standard_data[:]\n",
    "    #creating Y\n",
    "    #calculate daily % change using daily close using the NEXT day close / today close\n",
    "    df = pd.DataFrame()\n",
    "    target = data.close.shift(-1)/data.close-1\n",
    "    target[target > 0.01] = 1\n",
    "    target[target < -0.01] = -1\n",
    "    target[(target < 1) & (target > -1)] = 0\n",
    "    target += 1\n",
    "    df.insert(0,'target', target)\n",
    "    df.insert(1,'change', data.close.shift(-1)/data.close-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common rows with nan from full_data and target and return new dataset\n",
    "def remove_nan(full_data, target):\n",
    "    to_keep = [not x for x in np.array(list(map(any,full_data.isna().values))) | np.array(list(map(any,target.isna().values)))]\n",
    "    full_data = full_data[to_keep]\n",
    "    target = target[to_keep]\n",
    "    return full_data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_strategy(results, data, prefix = ''):\n",
    "    # the evaluation process would be a simulation of trading the stocks at the close.\n",
    "    #   when the prediction is:\n",
    "    #    0 : short/sell\n",
    "    #    1 : unclear thus liquidate and wait for long/short signals\n",
    "    #    2 : long/buy\n",
    "    #   when there are consective singnals of buy or sell, the action would be to hold\n",
    "    # assuming no commission per trade, and orders always fill at the close.\n",
    "    max_balance = 10000\n",
    "    eval_data = results.merge(data, left_index=True, right_index=True)\n",
    "    beginning_balance = [10000]\n",
    "    beginning_cash = [10000]\n",
    "    shares_owned = [0]\n",
    "    ending_balance = [10000]\n",
    "    ending_cash = [10000]\n",
    "    draw_down = [0]\n",
    "    actions = ['liquidate']\n",
    "    predictions = ['liquidate']\n",
    "    pred_to_action = {\n",
    "        0:'short',\n",
    "        1:'liquidate',\n",
    "        2:'long'\n",
    "    }\n",
    "    for idx, row in eval_data.iterrows():\n",
    "        beginning_balance.append(ending_balance[-1])\n",
    "        ending_balance.append(ending_cash[-1]+(shares_owned[-1])*row.close)\n",
    "        beginning_cash.append(ending_cash[-1])\n",
    "        ending_cash.append(ending_cash[-1])\n",
    "        shares_owned.append(shares_owned[-1])\n",
    "        # if same as previous\n",
    "        if pred_to_action[row.predictions.argmax()] == predictions[-1]:\n",
    "            action = 'hold'\n",
    "        # if liquidate\n",
    "        elif row.predictions.argmax() == 1:\n",
    "            ending_cash[-1] += shares_owned[-1] * row.close\n",
    "            shares_owned[-1] = 0\n",
    "            action = pred_to_action[row.predictions.argmax()]\n",
    "        # if long\n",
    "        elif row.predictions.argmax() == 2:\n",
    "            ending_cash[-1] += shares_owned[-1] * row.close\n",
    "            shares_owned[-1] = 0\n",
    "            shares_owned[-1] += np.floor(ending_cash[-1]/row.close)\n",
    "            ending_cash[-1] -= np.floor(ending_cash[-1]/row.close) * row.close\n",
    "            action = pred_to_action[row.predictions.argmax()]\n",
    "        # if short\n",
    "        elif row.predictions.argmax() == 0:\n",
    "            ending_cash[-1] += shares_owned[-1] * row.close\n",
    "            shares_owned[-1] = 0\n",
    "            shares_owned[-1] -= np.floor(ending_cash[-1]/row.close)\n",
    "            ending_cash[-1] -= -np.floor(ending_cash[-1]/row.close) * row.close\n",
    "            action = pred_to_action[row.predictions.argmax()]\n",
    "        #calc\n",
    "        max_balance = max(max_balance,ending_balance[-1])\n",
    "        draw_down.append(ending_balance[-1]/max_balance - 1)\n",
    "        actions.append(action)\n",
    "        predictions.append(pred_to_action[row.predictions.argmax()])\n",
    "    df = pd.DataFrame()\n",
    "    df.insert(0, 'beginning_balance', beginning_balance)\n",
    "    df.insert(1, 'beginning_cash', beginning_cash)\n",
    "    df.insert(2, 'shares_owned', shares_owned)\n",
    "    df.insert(3, 'ending_cash', ending_cash)\n",
    "    df.insert(4, 'ending_balance', ending_balance)\n",
    "    df.insert(5, 'draw_down', draw_down)\n",
    "    df.insert(6, 'actions', actions)\n",
    "    df.columns = [prefix+'_'+x if prefix else x for x in df.columns]\n",
    "    return df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_buy_and_hold(results, data, prefix = ''):\n",
    "    # the evaluation process would be a simulation of trading the stocks at the close and always long/short as much as possible and liquidate the next day\n",
    "    # assuming no commission per trade, and orders always fill at the close.\n",
    "    max_balance = 10000\n",
    "    eval_data = results.merge(data, left_index=True, right_index=True)\n",
    "    beginning_balance = [10000]\n",
    "    beginning_cash = [10000]\n",
    "    shares_owned = [0]\n",
    "    ending_balance = [10000]\n",
    "    ending_cash = [10000]\n",
    "    draw_down = [0]\n",
    "    actions = ['liquidate']\n",
    "    \n",
    "    for idx, row in eval_data.iterrows():\n",
    "        beginning_balance.append(ending_balance[-1])\n",
    "        ending_balance.append(ending_cash[-1]+(shares_owned[-1])*row.close)\n",
    "        beginning_cash.append(ending_cash[-1])\n",
    "        ending_cash.append(ending_cash[-1])\n",
    "        shares_owned.append(shares_owned[-1])\n",
    "        \n",
    "        #liquidate\n",
    "        ending_cash[-1] += shares_owned[-1] * row.close\n",
    "        shares_owned[-1] = 0\n",
    "        action = 'liquidate'\n",
    "        # long\n",
    "        shares_owned[-1] += np.floor(ending_cash[-1]/row.close)\n",
    "        ending_cash[-1] -= (ending_cash[-1]/row.close) * row.close\n",
    "        action = 'long'\n",
    "\n",
    "        #calc\n",
    "        max_balance = max(max_balance,ending_balance[-1])\n",
    "        draw_down.append(ending_balance[-1]/max_balance - 1)\n",
    "        actions.append(action)\n",
    "    df = pd.DataFrame()\n",
    "    df.insert(0, 'beginning_balance', beginning_balance)\n",
    "    df.insert(1, 'beginning_cash', beginning_cash)\n",
    "    df.insert(2, 'shares_owned', shares_owned)\n",
    "    df.insert(3, 'ending_cash', ending_cash)\n",
    "    df.insert(4, 'ending_balance', ending_balance)\n",
    "    df.insert(5, 'draw_down', draw_down)\n",
    "    df.insert(6, 'actions', actions)\n",
    "    df.columns = [prefix+'_'+x if prefix else x for x in df.columns]\n",
    "    return df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is a timeseries dataset and because of my personal trading experience I'm claming/assuming the underlying relationship between features and target variables isn't stationary, \n",
    "# thus the traning method will be in walk-forward style instead of cross-validation; and for each iteration the model isn't going to using everydata available since they aren't as relevant.\n",
    "# will try to play with weights in the future to see if applying less weights to \"outdated\" data will help the model.\n",
    "\n",
    "def train_and_eval(full_data, target):\n",
    "    training_window_size = 1500 # days of data to train the model for each iteration. using about 3 years of data\n",
    "    validation_window_size = 100\n",
    "    predict_window_size  = 1  # days of data used to test and eval the model for each iteration. Using about 2 weeks of data \n",
    "                               # ideally the predict_window_size should be set to 1, but that would take too long to train.\n",
    "                               # maybe I would try it when I have a good model with good parameters\n",
    "    best_iteration = 1\n",
    "    # check full_data len is more than training_window_size + predict_window_size if false throw error\n",
    "    assert len(full_data) > training_window_size + predict_window_size, \"full_data lenght is less than training_window_size + predict_window_size\"\n",
    "    predictions = []\n",
    "    truths = []\n",
    "    prediction_results = target[training_window_size:]\n",
    "    for i in range(training_window_size,len(full_data),predict_window_size):\n",
    "        validation_size = validation_window_size\n",
    "        # setup train and test data\n",
    "        train_x = full_data[i-training_window_size:i-validation_window_size]\n",
    "        valid_x = full_data[i-validation_window_size:i]\n",
    "        test_x  = full_data[i:i+predict_window_size]\n",
    "        \n",
    "        train_y = target[i-training_window_size:i-validation_window_size]\n",
    "        valid_y = target[i-validation_window_size:i]\n",
    "        test_y  = target[i:i+predict_window_size]\n",
    "        \n",
    "        # oversample trainning data to balance the dataset\n",
    "        oversample = SMOTE(k_neighbors = 5, random_state = 0) \n",
    "        train_x, train_y = oversample.fit_resample(train_x, train_y.target)\n",
    "        validation_set_skip = False\n",
    "        try:\n",
    "            valid_x, valid_y = oversample.fit_resample(valid_x, valid_y.target)\n",
    "            validation_data = lgb.Dataset(valid_x, label=valid_y, reference=train_data, free_raw_data=False)\n",
    "        except:\n",
    "            validation_set_skip = True # not enough samples vs neighbors, skipping validation for this iteration and keep using last iteration model parameters but trained with new data\n",
    "\n",
    "        # create lgb.Dataset for both train and test for lightgbm library use\n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "        \n",
    "        # setup lightgbm parameters\n",
    "        param = {'metric': 'multi_logloss', 'objective': 'multiclass', 'num_class':3}\n",
    "        param['learning_rate'] = 0.01\n",
    "        param['max_depth'] = 20\n",
    "        param['num_leaves'] = 20\n",
    "        param['min_data_in_leaf'] = 20\n",
    "        param['min_sum_hessian_in_leaf'] = 1e-3\n",
    "        param['bagging_fraction'] = 0.9\n",
    "        param['bagging_freq'] = 5\n",
    "        param['bagging_seed'] = 3\n",
    "        param['feature_fraction'] = 0.9\n",
    "        param['feature_fraction_bynode'] = 0.9\n",
    "        param['feature_fraction_seed'] = 2\n",
    "        param['lambda_l1'] = 0.01\n",
    "        param['lambda_l2'] = 0.01\n",
    "        param['force_col_wise'] = True\n",
    "        param['num_threads'] = 4\n",
    "        param['verbose'] = -1\n",
    "        \n",
    "        if not validation_set_skip:\n",
    "#             print('using validation data to find best iteration')\n",
    "            num_round = 1000\n",
    "            bst = lgb.train(param, train_data, num_round, valid_sets=[validation_data], early_stopping_rounds=5, verbose_eval=False)\n",
    "            best_iteration = bst.best_iteration\n",
    "#         print('using training only with last known good number of iterations')\n",
    "        num_round = best_iteration\n",
    "        train_x = full_data[i-training_window_size:i]\n",
    "        train_y = target[i-training_window_size:i]\n",
    "\n",
    "        train_x, train_y = oversample.fit_resample(train_x, train_y.target)\n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "        \n",
    "        bst = lgb.train(param, train_data, num_round, verbose_eval=False)\n",
    "        pred = bst.predict(test_x, num_iteration_predict = bst.best_iteration)\n",
    "        \n",
    "        predictions.extend(pred)\n",
    "        truths.extend(test_y)\n",
    "    prediction_results.insert(2, 'predictions', predictions)\n",
    "    \n",
    "    return bst, prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(standard_data):\n",
    "    \"\"\"\n",
    "    Draw interactive candle stick chart OHLC Volume\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    standard_data : pandas dataframe that contains ['date','high','low','close','open','volume'] columns.\n",
    "    \n",
    "    \"\"\"\n",
    "    qf = cf.QuantFig(standard_data,legend='bottom')\n",
    "    qf.add_volume()\n",
    "    qf.iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw data\n",
    "raw_data = MS.get('IWM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>adj_high</th>\n",
       "      <th>adj_low</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>adj_open</th>\n",
       "      <th>adj_volume</th>\n",
       "      <th>symbol</th>\n",
       "      <th>exchange</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>83.45</td>\n",
       "      <td>83.7700</td>\n",
       "      <td>83.0400</td>\n",
       "      <td>83.350</td>\n",
       "      <td>39513145.0</td>\n",
       "      <td>73.068149</td>\n",
       "      <td>72.431408</td>\n",
       "      <td>72.701805</td>\n",
       "      <td>72.789029</td>\n",
       "      <td>39513145.0</td>\n",
       "      <td>IWM</td>\n",
       "      <td>ARCX</td>\n",
       "      <td>2011-02-18T00:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>82.60</td>\n",
       "      <td>83.5100</td>\n",
       "      <td>82.4500</td>\n",
       "      <td>83.260</td>\n",
       "      <td>34891170.0</td>\n",
       "      <td>72.841364</td>\n",
       "      <td>71.916782</td>\n",
       "      <td>72.623302</td>\n",
       "      <td>72.047619</td>\n",
       "      <td>34891170.0</td>\n",
       "      <td>IWM</td>\n",
       "      <td>ARCX</td>\n",
       "      <td>2011-02-17T00:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>82.21</td>\n",
       "      <td>82.8200</td>\n",
       "      <td>82.1783</td>\n",
       "      <td>82.680</td>\n",
       "      <td>38284670.0</td>\n",
       "      <td>72.239514</td>\n",
       "      <td>71.679793</td>\n",
       "      <td>72.117399</td>\n",
       "      <td>71.707443</td>\n",
       "      <td>38284670.0</td>\n",
       "      <td>IWM</td>\n",
       "      <td>ARCX</td>\n",
       "      <td>2011-02-16T00:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>82.27</td>\n",
       "      <td>82.4886</td>\n",
       "      <td>81.8300</td>\n",
       "      <td>82.018</td>\n",
       "      <td>42908536.0</td>\n",
       "      <td>71.950451</td>\n",
       "      <td>71.375989</td>\n",
       "      <td>71.539971</td>\n",
       "      <td>71.759778</td>\n",
       "      <td>42908536.0</td>\n",
       "      <td>IWM</td>\n",
       "      <td>ARCX</td>\n",
       "      <td>2011-02-15T00:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>82.11</td>\n",
       "      <td>82.6100</td>\n",
       "      <td>82.0500</td>\n",
       "      <td>82.490</td>\n",
       "      <td>36730775.0</td>\n",
       "      <td>72.056342</td>\n",
       "      <td>71.567883</td>\n",
       "      <td>71.951672</td>\n",
       "      <td>71.620218</td>\n",
       "      <td>36730775.0</td>\n",
       "      <td>IWM</td>\n",
       "      <td>ARCX</td>\n",
       "      <td>2011-02-14T00:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       open     high      low   close      volume   adj_high    adj_low  \\\n",
       "2502  83.45  83.7700  83.0400  83.350  39513145.0  73.068149  72.431408   \n",
       "2503  82.60  83.5100  82.4500  83.260  34891170.0  72.841364  71.916782   \n",
       "2504  82.21  82.8200  82.1783  82.680  38284670.0  72.239514  71.679793   \n",
       "2505  82.27  82.4886  81.8300  82.018  42908536.0  71.950451  71.375989   \n",
       "2506  82.11  82.6100  82.0500  82.490  36730775.0  72.056342  71.567883   \n",
       "\n",
       "      adj_close   adj_open  adj_volume symbol exchange  \\\n",
       "2502  72.701805  72.789029  39513145.0    IWM     ARCX   \n",
       "2503  72.623302  72.047619  34891170.0    IWM     ARCX   \n",
       "2504  72.117399  71.707443  38284670.0    IWM     ARCX   \n",
       "2505  71.539971  71.759778  42908536.0    IWM     ARCX   \n",
       "2506  71.951672  71.620218  36730775.0    IWM     ARCX   \n",
       "\n",
       "                          date  \n",
       "2502  2011-02-18T00:00:00+0000  \n",
       "2503  2011-02-17T00:00:00+0000  \n",
       "2504  2011-02-16T00:00:00+0000  \n",
       "2505  2011-02-15T00:00:00+0000  \n",
       "2506  2011-02-14T00:00:00+0000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongkangzhao/miniconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n",
      "/Users/yongkangzhao/miniconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# runtimewarnings are produced from ta library but it's nothing to worry about for this project, will need to figure out a way to suppress this warning message.\n",
    "# data prep steps then\n",
    "# drop rows with nan\n",
    "# naturally the last row contains nan since we don't have info from tomorrow. so we are also dropping the last row.\n",
    "# but in production we will want to keep the last row so we can use it to make prediction\n",
    "# for modeling and evaluation purposes it's not useful\n",
    "# after this step the data is ready to use as training dataset\n",
    "data = raw_data_preprocessing(raw_data)\n",
    "target = get_target_variable(data)\n",
    "indicators = get_ta_indicators(data, 'daily')\n",
    "\n",
    "# concat/merge datasets to create full_data\n",
    "full_data = indicators\n",
    "\n",
    "# last step is to remove rows with nan. i.e. first few rows that don't have enough days of data to compute averages etc, and the last row without future data to compute the targer.\n",
    "full_data, target = remove_nan(full_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2d454a86133c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-1a774d0942ad>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(full_data, target)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   2618\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2619\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2620\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2621\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, prediction_results = train_and_eval(full_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_and_hold_result = eval_buy_and_hold(prediction_results, data)\n",
    "strategy_result = eval_strategy(prediction_results, data)\n",
    "print('Max Drawdown:')\n",
    "print('  - Buy and Hold:', str(round(100*min(buy_and_hold_result.draw_down),2))+'%')\n",
    "print('  - LGB Strategy:', str(round(100*min(strategy_result.draw_down),2))+'%')\n",
    "\n",
    "#align index number\n",
    "strategy_result.index = prediction_results.index\n",
    "buy_and_hold_result.index = prediction_results.index\n",
    "#insert based on index number\n",
    "def plot_result(data, benchmark, strategy):\n",
    "    data = data[:] # making a copy so original data isn't altered.\n",
    "    data.insert(0,'LGB_Strategy', strategy.ending_balance)\n",
    "    data.insert(0,'BNH_Strategy', benchmark.ending_balance)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=data.date[data.BNH_Strategy.notnull()], y=data.BNH_Strategy[data.BNH_Strategy.notnull()], mode='lines', name='Buy and Hold Strategy'))\n",
    "    fig.add_trace(go.Scatter(x=data.date[data.BNH_Strategy.notnull()], y=data.LGB_Strategy[data.BNH_Strategy.notnull()], mode='lines', name='Lightgbm Strategy'))\n",
    "    fig.show()\n",
    "plot_result(data, buy_and_hold_result, strategy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((data,strategy_result),axis=1).to_csv('./results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # holiday info\n",
    "# min_year = int(min(data['date'])[:4])-2\n",
    "# max_year = int(max(data['date'])[:4])+2\n",
    "# min_date = str(min_year)+'-01-01'\n",
    "# max_date = str(max_year)+'-12-31'\n",
    "# dates = pd.date_range(min_date,max_date).values\n",
    "# holidays = holidays.UnitedStates(years=range(min_year,max_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
